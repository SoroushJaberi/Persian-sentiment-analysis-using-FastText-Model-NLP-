{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "W-Pas8HJSEVO"
   },
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import hazm\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training dataset from 'final_preprocessed.csv'\n",
    "train_dataset = pd.read_csv('final_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o_3XKP9oSEaH",
    "outputId": "18e38272-6531-4e37-d601-59400f49b514"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 132102 entries, 0 to 132101\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   comment    132102 non-null  object\n",
      " 1   sentiment  132102 non-null  object\n",
      " 2   Cleaned    132102 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 3.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# Display dataset information\n",
    "train_dataset.pop('Unnamed: 0')\n",
    "train_dataset.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "0q4iyMaHSEe_",
    "outputId": "fc4b057b-16e5-40e1-8296-69a53ca05dfe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>132102</td>\n",
       "      <td>132102</td>\n",
       "      <td>132102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>132099</td>\n",
       "      <td>3</td>\n",
       "      <td>131608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>نسبت به قیمتش خوبه</td>\n",
       "      <td>negative</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>64631</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   comment sentiment Cleaned\n",
       "count               132102    132102  132102\n",
       "unique              132099         3  131608\n",
       "top     نسبت به قیمتش خوبه  negative    None\n",
       "freq                     2     64631     316"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display dataset statistics\n",
    "train_dataset.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_dc9UWJRSEhO",
    "outputId": "3c9517e3-de9b-4ad5-f49a-67fe929ecfac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64631\n",
      "63918\n",
      "3553\n"
     ]
    }
   ],
   "source": [
    "# Check the number of samples per sentiment category\n",
    "print(list(train_dataset['sentiment'] == 'negative').count(True))\n",
    "print(list(train_dataset['sentiment'] == 'positive').count(True))\n",
    "print(list(train_dataset['sentiment'] == 'neutral').count(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qoy236anbZEo",
    "outputId": "6608cf71-b4d7-4c40-f186-934c1a9cd020"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Check for null values in the 'Cleaned' column\n",
    "print(train_dataset['Cleaned'].isnull().sum())\n",
    "print(train_dataset['Cleaned'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kaxYTuPMbZHJ",
    "outputId": "514d5be9-0926-418b-e7a1-30686a5ea183"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative', 'positive', 'neutral']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get unique sentiment categories\n",
    "categories = train_dataset['sentiment'].unique().tolist()\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "FrablQx_bZJW"
   },
   "outputs": [],
   "source": [
    "# Create a new column 'sentiment_fasttext' and initialize it to None\n",
    "train_dataset['sentiment_fasttext'] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocessing part\n",
    "# # Load data\n",
    "# data = pd.read_csv('first_file.csv')\n",
    "\n",
    "# # List of Persian punctuations\n",
    "# punctuations = string.punctuation + \"٬\" + \"،\"\n",
    "\n",
    "# # Create a translator object to remove punctuations\n",
    "# translator = str.maketrans('', '', punctuations)\n",
    "\n",
    "# # List of Persian stop words\n",
    "# stopwords = hazm.stopwords_list()\n",
    "\n",
    "# # Create a lemmatizer object\n",
    "# lem = hazm.Lemmatizer()\n",
    "\n",
    "# # Create a normalizer object\n",
    "# norm = hazm.Normalizer()\n",
    "\n",
    "# # Create an empty DataFrame for preprocessing\n",
    "# preprocessed_dataset = pd.DataFrame(columns=['comment', 'sentiment'])\n",
    "\n",
    "# # Iterate over the data and preprocess\n",
    "# for index, row in data.iterrows():\n",
    "#     # Tokenize and lemmatize the text\n",
    "#     text = row['comment']\n",
    "#     text_tokenized = hazm.word_tokenize(text)\n",
    "#     text_lem = [lem.lemmatize(x) for x in text_tokenized]\n",
    "\n",
    "#     # Normalize the text\n",
    "#     text_norm = [norm.normalize(x) for x in text_lem]\n",
    "\n",
    "#     # Remove stop words\n",
    "#     clean_text = [x for x in text_norm if not x in stopwords]\n",
    "\n",
    "#     # Remove punctuations\n",
    "#     final_text = [x.translate(translator) for x in clean_text]\n",
    "\n",
    "#     # Add the preprocessed text to the preprocessed_dataset\n",
    "#     preprocessed_dataset.loc[index] = ({\n",
    "#         'comment': ' '.join(final_text),\n",
    "#         'sentiment': row['sentiment']\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vdq3LUaVhx3J",
    "outputId": "922f8477-5764-4c49-ddc1-1d0cc75e3f50"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "132102it [00:20, 6302.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# Add \"__label__\" to each comment for FastText training\n",
    "for index, p in tqdm(enumerate(train_dataset['Cleaned'])):\n",
    "  cat = train_dataset['sentiment'][index]\n",
    "  label = '__label__'+ str(cat)\n",
    "  new_sentiment = label + \" \" + str(p)\n",
    "  train_dataset['sentiment_fasttext'][index] = new_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the updated dataset\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OxN6Ua9Shx-L",
    "outputId": "0a244517-baf2-46dc-fe3d-5a260b974731"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__label__negative',\n",
       " 'کیفیت',\n",
       " 'غذا',\n",
       " 'متوسط',\n",
       " 'رو',\n",
       " 'به',\n",
       " 'پایین',\n",
       " 'بود',\n",
       " 'انگار',\n",
       " 'داخل',\n",
       " 'یه',\n",
       " 'رستوران',\n",
       " 'معمولی',\n",
       " 'غذا',\n",
       " 'خوردی',\n",
       " 'درحالی',\n",
       " 'که',\n",
       " 'امتیاز',\n",
       " 'رستوران',\n",
       " 'در',\n",
       " 'اسنپ',\n",
       " 'فود',\n",
       " '4٫3',\n",
       " 'بود']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenize a sample comment using FastText's tokenize function\n",
    "fasttext.tokenize(train_dataset['sentiment_fasttext'][7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "qxD70aBjhyBB"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into train and test sets\n",
    "X_train, X_test = train_test_split(train_dataset, test_size=0.2, random_state=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VVtmrCfzhyEx",
    "outputId": "4b9097db-3564-4f23-89eb-bc7a6aad1514"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105681\n",
      "26421\n"
     ]
    }
   ],
   "source": [
    "# Display the sizes of the train and test sets\n",
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "B5Qk3YakiqIa"
   },
   "outputs": [],
   "source": [
    "# Export the 'sentiment_fasttext' column as a .txt file for FastText\n",
    "X_train['sentiment_fasttext'].to_csv('train.txt',\n",
    "                                     sep=' ',\n",
    "                                     index=False,\n",
    "                                     header=False,\n",
    "                                     quoting=csv.QUOTE_NONE,\n",
    "                                     quotechar=\"\",\n",
    "                                     escapechar=\" \")\n",
    "\n",
    "X_test['sentiment_fasttext'].to_csv('test.txt',\n",
    "                                    sep=' ',\n",
    "                                    index=False,\n",
    "                                    header=False,\n",
    "                                    quoting=csv.QUOTE_NONE,\n",
    "                                    quotechar=\"\",\n",
    "                                    escapechar=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.8637068998145415\n",
      "Best parameters: {'lr': 0.1, 'epoch': 100, 'wordNgrams': 4}\n"
     ]
    }
   ],
   "source": [
    "# Define a grid of hyperparameters for FastText\n",
    "grid = {\n",
    "    'lr': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "    'epoch': [100, 200, 300],\n",
    "    'wordNgrams': [1, 2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "all_names = grid.keys()\n",
    "combinations = list(itertools.product(*(grid[Name] for Name in all_names)))\n",
    "\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "\n",
    "# Iterate over hyperparameter combinations\n",
    "for combination in combinations:\n",
    "    params = dict(zip(all_names, combination))\n",
    "    \n",
    "    # Train a FastText model with the current hyperparameters\n",
    "    model = fasttext.train_supervised('train.txt',\n",
    "                                      lr=params['lr'],\n",
    "                                      epoch=params['epoch'],\n",
    "                                      wordNgrams=params['wordNgrams'],\n",
    "                                      loss='ns',\n",
    "                                      seed=123,\n",
    "                                      label='__label__')\n",
    "    \n",
    "    # Test the model on the test set\n",
    "    result = model.test('test.txt')\n",
    "    precision = result[1]\n",
    "    \n",
    "    # Update the best score and best parameters if needed\n",
    "    if precision > best_score:\n",
    "        best_score = precision\n",
    "        best_params = params\n",
    "\n",
    "# Display the best hyperparameters and score\n",
    "print(f'Best score: {best_score}')\n",
    "print(f'Best parameters: {best_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "paoGFu1QiqNN",
    "outputId": "717398d5-448f-47fd-8e0e-039bdb6bbf4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26421, 0.8629877748760456, 0.8629877748760456)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a FastText model with the best hyperparameters\n",
    "model = fasttext.train_supervised('train.txt',\n",
    "                                  wordNgrams=4,\n",
    "                                  lr=0.1,\n",
    "                                  epoch=100,\n",
    "                                  loss='ns',\n",
    "                                  seed=123,\n",
    "                                  label='__label__')\n",
    "\n",
    "# Test the model on the test set\n",
    "model.test('test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "K9H1TU7tiqP0"
   },
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model.save_model('fasttext_with_tune_data_model.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A9PZi0UMiqUx",
    "outputId": "01eb8df0-038b-47e7-c984-1722e86cb8a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26421, 0.8629877748760456, 0.8629877748760456)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved model and test it again\n",
    "loaded_model = fasttext.load_model('fasttext_with_tune_data_model.bin')\n",
    "loaded_model.test('test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "VJ-1_ixzl2RV"
   },
   "outputs": [],
   "source": [
    "# Load another test data for cheking prediction from 'total_preprocessed.csv'\n",
    "test_data = pd.read_csv('total_preprocessed.csv')\n",
    "\n",
    "# List of Persian punctuations\n",
    "punctuations = string.punctuation + \"٬\" + \"،\"\n",
    "\n",
    "# Create a translator object to remove punctuations\n",
    "translator = str.maketrans('', '', punctuations)\n",
    "\n",
    "# List of Persian stop words\n",
    "stopwords = hazm.stopwords_list()\n",
    "\n",
    "# Create a lemmatizer object\n",
    "lem = hazm.Lemmatizer()\n",
    "\n",
    "# Create a normalizer object\n",
    "norm = hazm.Normalizer()\n",
    "\n",
    "# Create an empty DataFrame for the test dataset\n",
    "test_dataset = pd.DataFrame(columns=['local_id', 'Cleaned'])\n",
    "\n",
    "# Iterate over the test data\n",
    "for index, row in test_data.iterrows():\n",
    "    # Tokenize and lemmatize the text\n",
    "    text = row['Cleaned']\n",
    "    text_tokenized = hazm.word_tokenize(text)\n",
    "    text_lem = [lem.lemmatize(x) for x in text_tokenized]\n",
    "\n",
    "    # Normalize the text\n",
    "    text_norm = [norm.normalize(x) for x in text_lem]\n",
    "\n",
    "    # Remove stop words\n",
    "    clean_text = [x for x in text_norm if not x in stopwords]\n",
    "\n",
    "    # Remove punctuations\n",
    "    final_text = [x.translate(translator) for x in clean_text]\n",
    "\n",
    "    # Add the preprocessed text to the test_dataset\n",
    "    test_dataset.loc[index] = ({\n",
    "        'Cleaned': ' '.join(final_text),\n",
    "        'local_id': row['local_id']\n",
    "    })\n",
    "\n",
    "# Display the test dataset\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "QOG1vDxKmwFj"
   },
   "outputs": [],
   "source": [
    "# Define a function for batch prediction using the trained model\n",
    "def bunch_predict(comments, model):\n",
    "    labels = []\n",
    "    probs = []\n",
    "\n",
    "    for comment in comments:\n",
    "        result = model.predict(comment)\n",
    "        labels.append(result[0][0].replace('__label__', ''))\n",
    "        probs.append(result[1][0])\n",
    "\n",
    "    return labels, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "ek4R-g7ZmwH7"
   },
   "outputs": [],
   "source": [
    "# Get 'local_id' and 'Cleaned' columns from the test dataset\n",
    "local_id = test_dataset['local_id']\n",
    "comments = test_dataset['Cleaned'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "xHgDI34CmwKL"
   },
   "outputs": [],
   "source": [
    "# Perform batch prediction on the test dataset\n",
    "labels, probs = bunch_predict(comments, model)\n",
    "\n",
    "# Create a new test dataset with predicted sentiments\n",
    "new_test = {\"local_id\": [], \"Cleaned\": [], \"sentiment\": []}\n",
    "n = 0\n",
    "\n",
    "for label, comment in zip(labels, comments):\n",
    "    new_test['Cleaned'].append(comment)\n",
    "    new_test['sentiment'].append(label)\n",
    "    new_test['local_id'].append(local_id[n])\n",
    "    n = n + 1\n",
    "\n",
    "# Create a DataFrame from the new test dataset\n",
    "df = pd.DataFrame(new_test)\n",
    "\n",
    "# Apply conditions to create new columns for each sentiment category\n",
    "df['negative'] = df['sentiment'].apply(lambda x: 1 if x == \"negative\" else 0)\n",
    "df['neutral'] = df['sentiment'].apply(lambda x: 1 if x == \"neutral\" else 0)\n",
    "df['positive'] = df['sentiment'].apply(lambda x: 1 if x == \"positive\" else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'sentiment' column\n",
    "df.drop('sentiment', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "IiHtDnZXmwPL"
   },
   "outputs": [],
   "source": [
    "# Save the final test dataset to 'total_testing.csv'\n",
    "df.to_csv('total_testing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "RQWDtQqx-NOS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
